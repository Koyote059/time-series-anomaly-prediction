# PatchTST Model Training Configuration

[training]
max_epochs = 100                ; Maximum number of training epochs
batch_size = 32                 ; Number of samples in each training batch
precision = 32                  ; Numerical precision for training (options: 32, 16, mixed)
accumulate_grad_batches = 1     ; Number of batches to accumulate gradients over
val_check_interval = 1.0        ; Validation check frequency (1.0 = once per epoch, can be ; 1.0)
log_every_n_steps = 50          ; Number of training steps between logging updates
num_workers = 1                 ; Number of workers to use for training
gradient_clip_value = None      ; Numerical value of the gradient clip

[early_stopping]
monitor = val_loss              ; Metric to monitor (options: val_loss, val_existence, val_density, val_leadtime, val_dice)
patience = 15                   ; Number of epochs to wait for improvement before stopping
mode = min                      ; Whether to minimize or maximize the metric (options: min, max)
min_delta = 1e-4               ; Minimum change to qualify as an improvement
verbose = true                 ; Whether to log early stopping information or not

[model_checkpoint]
save_directory = checkpoints/   ; Directory to save model checkpoints
filename = patchtst-{epoch:02d}-{val_loss:.4f}  ; Format for checkpoint filenames
monitor = val_loss             ; Metric to monitor for saving checkpoints
mode = min                     ; Whether to minimize or maximize the metric (options: min, max)
save_top_k = 3                 ; Number of best models to keep
save_last = true              ; Whether to additionally save the last model

[model]
threshold = 0.5                ; Threshold for binary classification decisions (0.0 to 1.0)

[metrics]
train_metrics_threshold = 0.5   ; Threshold to use for training metrics (0.0 to 1.0)
val_metrics_threshold = 0.5     ; Threshold to use for validation metrics (0.0 to 1.0)
evaluate_metrics_threshold = 0.5 ; Threshold to use for evaluation metrics (0.0 to 1.0)

[hardware]
num_devices = 1                ; Number of GPUs/devices to use
accelerator = auto             ; Hardware accelerator to use (options: auto, gpu, cpu, tpu)

[logging]
enabled = true                 ; Whether to enable logging (options: true, false)
save_directory = logs/         ; Directory to save logs

[scheduler]
type = ReduceLROnPlateau      ; Learning rate scheduler type (options: ReduceLROnPlateau, StepLR, CosineAnnealingLR, OneCycleLR, ExponentialLR, CosineAnnealingWarmRestarts)
monitor = val_loss            ; Metric to monitor for scheduling
frequency = 1                 ; How often to update the learning rate (in epochs)

[scheduler.ReduceLROnPlateau]
mode = min                    ; Options: min (for loss/error), max (for accuracy)
factor = 0.1                 ; Factor to decrease learning rate by (e.g., 0.1 = reduce by 10x)
patience = 10                ; Number of epochs with no improvement before reducing LR
verbose = true               ; If True, prints message when LR is reduced
threshold = 1e-4             ; Minimum change to qualify as an improvement
threshold_mode = rel         ; Options: rel (relative), abs (absolute)
cooldown = 0                 ; Epochs to wait before resuming normal operation after LR reduction
min_lr = 1e-8               ; Lower bound on learning rate
eps = 1e-8                  ; Minimal decay applied to lr

[scheduler.StepLR]
step_size = 30              ; Period of learning rate decay (epochs)
gamma = 0.1                 ; Multiplicative factor of learning rate decay

[scheduler.CosineAnnealingLR]
max_iter = 100              ; Maximum number of iterations
min_lr = 0                  ; Minimum learning rate

[scheduler.ExponentialLR]
gamma = 0.95                ; Multiplicative factor of learning rate decay

[scheduler.CosineAnnealingWarmRestarts]
T_0 = 50                    ; Number of iterations for the first restart
T_mult = 2                  ; Factor increasing T_i after a restart
min_lr = 1e-6               ; Minimum learning rate

[scheduler.OneCycleLR]
max_lr = 0.1                ; Upper learning rate boundary
total_steps = 1000          ; Total number of training steps
pct_start = 0.3             ; Percentage of cycle spent increasing LR (0.0-1.0)
anneal_strategy = cos       ; Options: cos, linear
cycle_momentum = true       ; If True, momentum cycling is enabled
base_momentum = 0.85        ; Lower momentum boundary
max_momentum = 0.95         ; Upper momentum boundary
div_factor = 25.0           ; Initial LR division factor
final_div_factor = 1e4      ; Final LR division factor


[optimizer]
type = Adam                    ; Optimizer algorithm (options: Adam, SGD, RMSprop, AdamW, Adadelta, Adagrad, RAdam)

[optimizer.Adam]
lr = 1e-3                     ; Initial learning rate
betas = (0.9, 0.999)          ; Coefficients for computing running averages of gradient and its square
eps = 1e-8                    ; Term added to denominator for numerical stability
weight_decay = 0.01           ; L2 regularization factor (0.0 means no regularization)
amsgrad = false               ; Whether to use the AMSGrad variant of Adam

[optimizer.AdamW]
lr = 1e-3                     ; Initial learning rate
betas = (0.9, 0.999)          ; Coefficients for computing running averages of gradient and its square
eps = 1e-8                    ; Term added to denominator for numerical stability
weight_decay = 0.01           ; Weight decay coefficient (different implementation from Adam)
amsgrad = false               ; Whether to use the AMSGrad variant

[optimizer.SGD]
lr = 1e-3                     ; Initial learning rate
momentum = 0.0                ; Momentum factor for SGD
dampening = 0.0              ; Dampening for momentum
nesterov = false             ; Whether to use Nesterov momentum
weight_decay = 0.0           ; Weight decay (L2 penalty)

[optimizer.RMSprop]
lr = 1e-3                     ; Initial learning rate
alpha = 0.99                 ; Smoothing constant
eps = 1e-8                  ; Term added to denominator for numerical stability
weight_decay = 0.0          ; Weight decay (L2 penalty)
momentum = 0.0              ; Momentum factor
centered = false            ; If True, compute the centered RMSprop

[optimizer.Adadelta]
lr = 1e-3                     ; Initial learning rate
rho = 0.9                   ; Coefficient used for computing running averages of squared gradients
eps = 1e-6                 ; Term added to denominator for numerical stability
weight_decay = 0.0         ; Weight decay (L2 penalty)

[optimizer.Adagrad]
lr = 1e-3                     ; Initial learning rate
lr_decay = 0.0             ; Learning rate decay
weight_decay = 0.0         ; Weight decay (L2 penalty)
initial_accumulator_value = 0.0  ; Initial value for accumulator
eps = 1e-10               ; Term added to denominator for numerical stability

[optimizer.RAdam]
lr = 1e-3                     ; Initial learning rate
betas = (0.9, 0.999)       ; Coefficients for computing running averages
eps = 1e-8                ; Term added to denominator for numerical stability
weight_decay = 0.0        ; Weight decay (L2 penalty)