[data]
enc_in = 10    ; Number of input variables/features in the time series data

[seq]
len = 100      ; Length of the input sequence window used for making predictions
max_len = 1024 ; Maximum allowed sequence length to prevent memory issues

[pred]
len = 100      ; Number of future time steps to predict

[patch]
len = 16       ; Size of each patch segment the input sequence is divided into
stride = 8     ; Steps between consecutive patch starts (smaller = more overlap)
padding = true ; Whether to pad sequences shorter than patch_len to match size
padding_var = -1 ; Value used for padding invalid/missing positions

[encoder]
layers = 1     ; Number of transformer encoder layers in the stack
heads = 4      ; Number of parallel self-attention heads per layer
dim = 128      ; Hidden dimension size for transformer representations
ff_dim = 256   ; Size of the transformer's feed-forward network
individual = false ; If true, use separate encoders for each input feature

[classifier]
layers_sizes = (128,128) ; Dense layer sizes for final prediction (2 layers of 128 units)
window_size = 5     ; Number of time steps considered in final prediction layer

[classifier.activation]
hidden = gelu    ; Activation function for intermediate classifier layers
output = sigmoid ; Activation function for final output layer (0-1 range for anomalies)

[attn]
key_dim = 32    ; Dimension of key vectors in self-attention
value_dim = 32  ; Dimension of value vectors in self-attention
dropout = 0.0   ; Probability of dropping attention weights during training
residual = true ; Whether to add residual connections around attention blocks
store = false   ; Whether to save attention weights for later visualization
mask = auto     ; Attention mask type: auto/subsequent/causal/None

[dropout]
main = 0.1      ; Global dropout rate applied throughout model
fc = 0.1        ; Dropout rate for fully connected layers
head = 0.1      ; Dropout rate for attention heads
classifier = 0.1 ; Dropout rate in classifier layers

[norm]
type = BatchNorm ; Normalization layer type: BatchNorm/LayerNorm/InstanceNorm
revin = true    ; Whether to use Reversible Instance Normalization
pre = false     ; If true, normalize before operations instead of after
affine = true   ; Whether to learn scaling and bias in normalization
subtract_last = false ; Whether to subtract last value for relative processing

[decomp]
use = true      ; Whether to decompose input into trend and seasonal components
kernel_size = 5 ; Kernel size for decomposition (must be odd number)

[pos]
encoding = zeros ; Initial positional encoding: zeros/uniform/normal/sincos
learnable = true ; Whether position encodings can be learned during training

[head]
type = flatten  ; How to process final features: flatten/avg/max/conv/attention
pretrain = false ; Whether to initialize head with pretrained weights

[act]
type = gelu     ; Default activation function used throughout model